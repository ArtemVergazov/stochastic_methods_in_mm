{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: MLE with scipy.minimize\n",
    "\n",
    "One needs to find which source distribution was most likely used to generate `unknown_sample.txt`\n",
    "\n",
    "- Gamma distribution, where $k$ is shape, $\\theta = 1$ is scale, $\\Gamma$ is Gamma function\n",
    "\n",
    "$$p_{gamma}(x) = x^{k-1} \\ \\frac{e^{-x/\\theta}}{\\theta^k \\ \\Gamma(k)}$$\n",
    "\n",
    "\n",
    "- or Gumbel distribution, where $\\mu$ is the mode, and $\\beta=1$ is the scale\n",
    "\n",
    "$$ p_{gumbel}(x) = \\frac{e^{-(x - \\mu)/ \\beta}}{\\beta} e^{ -e^{-(x - \\mu)/ \\beta}} $$\n",
    "\n",
    "\n",
    "In this task we ask you to use `scipy.minimize` to maximize $\\log L$, i.e. minimize its negative value.\n",
    "Scale parameters $\\theta$ and $\\beta$ here are constant and equal to 1 for both distributions, so you can simpify PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restoring the true distribution\n",
    "Write down the likelihood $L$ and  the negative log-likelihood  for $p_{gamma}(x, k)$, $p_{gumbel}(x, \\mu)$ and sample $x = \\{x_0, ..., x_n\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below sample `x` is already imported as a global variable. Implement `neg_loglikelihood_gamma` as a function of $k$ and `neg_loglikelihood_gumbel` as a function of $\\mu$. Use numpy and scipy.special where needed.\n",
    "\n",
    "Run the minimizer with given initial parameters using this call\n",
    "\n",
    "    result = minimize(func, init_param, method = 'Nelder-Mead', options={'disp': True})\n",
    "\n",
    "Check the result. Answer the following questions\n",
    "\n",
    "1. What is the most probable value of shape $\\hat{k}$ if the distribution was $p_{gamma}$?\n",
    "\n",
    "\n",
    "2. What is the most probable value of mode $\\hat{\\mu}$ if the distribution was $p_{gumbel}$?\n",
    "\n",
    "\n",
    "3. Which distribution has the highest probability to be the true one?\n",
    "\n",
    "Ensure that your solution is correct by plotting both PDFs with found $\\hat{k}$ and $\\hat{\\mu}$ over the histogram of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.loadtxt('unknown_sample.txt')\n",
    "\n",
    "\n",
    "gamma_k0 = 0.1\n",
    "gumbel_mu0 = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation\n",
    "\n",
    "Some optimization methods like BFGS (Broyden–Fletcher–Goldfarb–Shanno) use first and second order derivatives of given function. To simplify calculations of gradients, jacobians and hessians, we are going to employ an autograd package, able to differentiate native python and numpy code. \n",
    "\n",
    "Package: https://github.com/HIPS/autograd\n",
    "\n",
    "This package is a \"drop-in\" replacement for many numpy and some scipy methods. To check what special functions are supported you need to look through [the source code](https://github.com/HIPS/autograd/blob/master/autograd/scipy/special.py) of correspondent module.\n",
    "\n",
    "Automatic differentiation (autograd) mechanics is not a numerical approximations (a.k.a finite difference methods). Instead of this, using chain rules of differentiation it tracks every elementary operation performed on input data and stores a gradient as a numerical value.\n",
    "\n",
    "- examples of computational graphs\\\n",
    "  https://en.wikipedia.org/wiki/Automatic_differentiation\n",
    "  \n",
    "\n",
    "- explanation of autograd mechanics\\\n",
    "  https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#whats-going-on-under-the-hood\n",
    "  \n",
    "\n",
    "There are other framework with similar abilities, for instance \n",
    "[PyTorch](https://pytorch.org/), \n",
    "[JAX](https://github.com/google/jax) (successor of autograd), \n",
    "[TensorFlow](https://www.tensorflow.org/), \n",
    "[Theano](https://github.com/Theano/Theano), \n",
    "[MATLAB](https://www.mathworks.com/help/deeplearning/ug/include-automatic-differentiation.html) and even\n",
    "[StalinGRAD](https://github.com/Functional-AutoDiff/STALINGRAD) :)\n",
    "\n",
    "To install autograd in your conda/venv environment (my is called `base`) run\n",
    "\n",
    "`(base) $ pip install autograd`\n",
    "\n",
    "Look through example below and check that native differentiation works with Cobb-Douglas production function\n",
    "\n",
    "$$ f(x, y) = x^{0.8} \\ y^{0.2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "\n",
    "def f(x, y):\n",
    "    return 0  # your implementation here\n",
    "\n",
    "def f_x(x, y):\n",
    "    return 0  # your implementation here\n",
    "\n",
    "def f_y(x, y):\n",
    "    return 0  # your implementation here\n",
    "\n",
    "\n",
    "# first derivatives for f(x,y), x is position 0 (default) and y is 1\n",
    "dfdx = grad(f)\n",
    "dfdy = grad(f, 1)\n",
    "\n",
    "# suppose values for x and y are as follows\n",
    "x, y = 2.0, 3.0\n",
    "\n",
    "# evaluate the grads at x, y\n",
    "print(f\"autograd : {[dfdx(x,y), dfdy(x,y)]}\")\n",
    "\n",
    "# compare with analytical derivatives\n",
    "print(f\"analytical: {[f_x(x,y), f_y(x,y)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function using only np & scipy fundamental methods will make it autograd ready. Note that we need to run import statements again to rewrite references.\n",
    "\n",
    "Take the code from the previous part and run BFGS optimization in the block below by calling\n",
    "    \n",
    "    minimize(func, init_param, jac=None, method = 'BFGS', options={'disp': True})\n",
    "                     \n",
    "Then provide optimizer with jacobians instead of `jac=None`. Use `jacobian` and `hessian` functions in the same way with `grad`.\n",
    "\n",
    "Answer the following questions\n",
    "\n",
    "4. Is there any difference between running the BFGS optimizer with and without a jacobian?\n",
    "\n",
    "\n",
    "5. Do you spot any changes in an iterations number between BFGS and Nelder-Mead optimizers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd.scipy as scipy\n",
    "from autograd import jacobian, hessian\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('aqua-predict')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2120b70050a4bc3015589e0298cdda1a4249879b1ba9304472013b0b8792895a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
