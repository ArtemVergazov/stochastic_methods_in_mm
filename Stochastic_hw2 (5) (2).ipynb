{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NKz4Aj4TH8-"
      },
      "source": [
        "# 1. Fluctuations and KL divergence\n",
        "\n",
        "Let’s consider a general physical system, described by a probability density $P^a(ω)$, where ω denotes the degrees of freedom of the system.  For example,  we may take ω=x to be the coordinates of a diffusing  particle  and $P^a(ω)$  to  be  the  probability  density  of  the  particle’s  position  at  time  t.   We further specify an observable $r(ω)$.  Depending on ω, such an observable may, e.g., be a function of the  particle’s  position.   We  denote  the  average  of r(ω)  by $<r>^a=\\int_\\Omega dωr(ω)P^a(ω)$, provided that $\\Omega$ is the support of the distribution, and $r(ω)P^a(ω)$ is an integrable function. Although I am not familiar with strict probability definitions, I will hazard a guess that since supports of distributions seem to be well describable by measure theory, we demand at least an integral in Lebesgue sense to be finite over the domain.   Since ω is a random variable, the observable $r(ω)$ will likewise fluctuate.  We can characterize the fluctuations of $r(ω)$ by its deviations from the average $\\Delta r(ω) =r(ω)−<r>^a$.  We now perturb the system, e.g., by applying an external force.  The perturbation changes the probability density $P_b(ω)$ and the average of the observable to $<r>^b$.  We refer to $a$ and $b$ as the reference and perturbed systems, respectively. Provided that $P_a(ω)$ and $P_b(ω)$ have the same support (i.e.,$\\frac{P_a(\\omega)}{P_b(\\omega)}$is finite for all ω):\n",
        "\n",
        "1. First write down the cumulant generating function of fluctuations $\\Delta r(ω)$ in the reference system.\n",
        "2. Use  the  fact  that $P_a(ω)$ and $P_b(ω)$ have the same support and  rewrite  cumulant  generating function.\n",
        "3. Apply the Jensen’s inequality to cumulant generating function (Since the logarithm is a concave function, you are eligible) .\n",
        "4. Find the relation between response of the system to the perturbation $<r>^b−<r>^a$ and KL divergence.\n",
        "5. Apply your results to the special case when $r$ follows a Gaussian distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhkyZJcDWRNC"
      },
      "source": [
        "## Solution\n",
        "\n",
        "1.1. \n",
        "We first obtain the characteristic function of the fluctuations by way of the definition: \n",
        "\n",
        "Let $q_a(\\omega) = \\Delta r(ω) =r(ω)−<r>^a$\n",
        "\n",
        "We know by definition that $\\hat{r}_a = <r>^a=\\int_\\Omega dωr(ω)P^a(ω)$\n",
        "\n",
        "By virtue of considering a general system, we may not pinpoint its form more further, only mentioning that by assumption of integrability the whole utterance is finite. Let us now utilise the definition of the characteristic function of some distribution, noting that since the expectation is a single finite numerical value, and $\\omega$ is a random variable, $q_a$ is also a random variable, which has the support $\\Omega_1 = q_a(\\Omega)$. We now study this variable in more detail.\n",
        "\n",
        "With very little loss of generality we set $r$ and, by extension, $q$ as monotonic. Nonmonotonic functions can be obtained by partitioning the domain and considering each monotonic piece as a function, then summing the terms. Let $f_q(q)$ be the PDF of $q_a$. Then $$f_q(q(\\omega)) = \\frac{P^a(ω)}{|r'_\\omega (\\omega)|}$$\n",
        "\n",
        "Let $r$ be increasing wrt $\\omega$ for ease of writing further; decreasing functions then merit a minus sign.\n",
        "\n",
        "Thus we recover the PDF of $q$ in terms of pdf of $\\omega$, $$f_q(q(\\omega)) = \\frac{P^a(ω)}{r'_\\omega (\\omega)}$$\n",
        "\n",
        "These assumptions were a bit stringent, but necessary to reduce clutter. I hope I may be forgiven. An additional assumption, initially left implicit, is the smoothness of $r$. \n",
        "\n",
        "We next write the definition of the characteristic function of $q_a$:\n",
        "\n",
        "$$G_q(k) = <e^{ikq}> = \\int_{\\Omega_1} dq(\\omega)e^{ikq(\\omega)}f_q(q(\\omega)) = \\int_{\\Omega} r'_\\omega (\\omega)dωe^{ikq(\\omega)}\\frac{P^a(ω)}{r'_\\omega (\\omega)} = \\int_{\\Omega} dωe^{ikq(ω)}P^a(ω)$$\n",
        "\n",
        "Next we recall the deifnition of the cumulant generating function:     \n",
        "\n",
        "$$K_{q_a}(k) = \\log{G_q(k)} = \\log{\\int_{\\Omega} dωe^{ikq(ω)}P^a(ω)} = \\log{\\int_{\\Omega} dωe^{ik(r(ω)−<r>^a)}P^a(ω)} = \\log{\\int_{\\Omega} dω\\frac{e^{ikr(ω)}}{e^{ik<r>^a}}P^a(ω)} = \\log\\frac{1}{e^{ik<r>^a}}{\\int_{\\Omega} dωe^{ikr(ω)}P^a(ω)} = \\log{\\int_{\\Omega} dωe^{ikr(ω)}P^a(ω)} - ik<r>^a = K_{r_a}(k) - ik<r>^a$$\n",
        "\n",
        "NB: it is true that you've told me that doing so (introducing constraints and variables) will not yield the correct answer. However, I have specifically obtained Mr. Palyulin's permission to assume monotonic observable, as nonmonotonic observables are sums of monotonic ones on intervals of monotonic behavior; the extra variabe is a purely bookeeping device that does not affect the reasoning, and without treating the map $q$ (or $r - <r>^a)$ with the method of inverse transform it is quite impossible to express $K$ in terms of the PDF of $\\omega$. This, indeed, was the whole reason for both the new variable and the constraints.\n",
        "\n",
        "1.2. \n",
        "\n",
        "I still have absolutely no idea what is rewriting the CDF supposed to mean here, as CDF of reference system is wholly unconnected with the parameters of the perturbed one in terms of direct dependence. Since the perturbation is not even explicitly stated as small, I cannot expand in terms of small parameter to establish the connection, so the best guess I have is just to write the CDF for the perturbed system. \n",
        "\n",
        "\n",
        "$$K_{q_b}(k) = \\log{\\int_{\\Omega} dωe^{ik(r(ω)−<r>^b)}P^b(ω)} = K_{r_b}(k) - ik<r>^b$$\n",
        "\n",
        "This is obtained by way of the exact same procedure, and the domain of integration is kept as per the given information. This task vexes me greatly.\n",
        "\n",
        "Coming back from pondering 1.4., I have divined the meaning of this task, I think. It is by far the hardest one to understand what is being demanded until task 1.4. is considered. \n",
        "\n",
        "$$K_{q_a}(k) = \\log{\\int_{\\Omega} dωe^{ik(r(ω)−<r>^a)}P^a(ω)} = \\log{\\int_{\\Omega} dωe^{ik(r(ω)−<r>^a)}\\frac{P^a(ω)P^b(ω)}{P^b(ω)}} = \\log{\\int_{\\Omega} dωe^{ik(r(ω)−<r>^a)}P^b(ω)\\frac{P^a(ω)}{P^b(ω)}}$$\n",
        "\n",
        "1.3. \n",
        "\n",
        "The Jensen's inequality reads (for concave $f$): $<f(x)>\\le f(<x>)$.\n",
        "Applying it to the CDF of unperturbed system yields:    \n",
        "$$\\log{\\int_{\\Omega} dωe^{ik(r(ω)−<r>^a)}P^a(ω)} = \\log\\mathbb{E}[e^{ik(r(ω)−<r>^a)}]\\hspace{2 mm} \\ge \\hspace{2 mm}\\mathbb{E}[ik(r(ω)−<r>^a)] \\hspace{2 mm}= \\hspace{2 mm}ik(\\mathbb{E}[r(ω)]−\\mathbb{E}[<r>^a]) \\hspace{2 mm}=\\hspace{2 mm} ik\\mathbb{E}[r(ω)] - ik<r>^a \\hspace{2 mm}=\\hspace{2 mm} 0$$\n",
        "\n",
        "The result above is shockingly uninteresting, which makes sense considering I have misunderstood task 1.2.\n",
        "\n",
        "Apply instead to result of 1.2.:     \n",
        "\n",
        "$$\\log{\\int_{\\Omega} dωe^{ik(r(ω)−<r>^a)}P^b(ω)\\frac{P^a(ω)}{P^b(ω)}}\\hspace{2 mm}\\ge\\hspace{2 mm}  {\\int_{\\Omega} dωP^b(ω)\\log[e^{ik(r(ω)−<r>^a)}\\frac{P^a(ω)}{P^b(ω)}}] \\hspace{2 mm}=\\hspace{2 mm} {\\int_{\\Omega} dω[ikr(ω)P^b(ω)−ikP^b(ω)<r>^a+P^b(ω)\\log\\frac{P^a(ω)}{P^b(ω)}}] \\hspace{2 mm}=\\hspace{2 mm} {\\int_{\\Omega} dω[ikr(ω)P^b(ω)−ikP^b(ω)<r>^a-P^b(ω)\\log\\frac{P^b(ω)}{P^a(ω)}}] \\hspace{2 mm}=\\hspace{2 mm} ik(<r>^b - <r>^a) - D_{KL}(b\\|a)$$\n",
        "\n",
        "\n",
        "1.4.\n",
        "\n",
        "Accidentally did it above.\n",
        "\n",
        "\n",
        "To put it more explicitly, by using the fact that we established in 1.3.,  $$ik(<r>^b - <r>^a) \\le D_{KL}(b\\|a) + K_{q_a}(k)$$\n",
        "\n",
        "1.5. \n",
        "\n",
        "Let $r$ itself follow a Gaussian distribution, noting also that if we consider $r$, its mean will be precisely $<r>$, and its variance by definition of variance will be $<(r-<r>)^2> = <\\Delta r^2>$:     \n",
        "$$P_{r} = \\frac{1}{\\sqrt{2\\pi<\\Delta r^2>}}exp(-\\frac{(r-<r>)^2}{2<\\Delta r^2>})$$\n",
        "\n",
        "This distribution holds for both reference and perturbed systems, hence the absence of any demarkation. \n",
        "\n",
        "First write the Gaussian cumulant generating function.\n",
        "\n",
        "$$K_{r_{gauss}}(k) = \\log\\int_R dre^{ikr}P(r) = \\log\\int_R dre^{ikr}\\frac{1}{\\sqrt{2\\pi<\\Delta r^2>}}exp(-\\frac{(r-<r>)^2}{2<\\Delta r^2>}) =  \\log\\frac{1}{\\sqrt{2\\pi<\\Delta r^2>}}\\int_R dre^{ikr-\\frac{(r-<r>)^2}{2<\\Delta r^2>}}$$\n",
        "\n",
        "Further integration with the desire to find the characteristic function is reliant on a lot of clunky algebra and will be submitted as an appendix in the form of a photo of a handwritten derivation. Here I instead state the following: \n",
        "$$G_{r_{gauss}}(k) = e^{i<r>k-\\frac{<\\Delta r^2>k^2}{2}}$$\n",
        "\n",
        "Therefore, $$K_{r_{gauss}}(k) = \\log G_{r_{gauss}}(k) = i<r>k-\\frac{<\\Delta r^2>k^2}{2}$$.\n",
        "\n",
        "This immediately shows that only two cumulants exist for the Gaussian (to be precise, I mean two not identically zero cumulants).\n",
        "\n",
        "We cannot use the cumulant generating function for raw $r$, since our result is in terms of $K$ of the deviation. It is evident that $$K_{g_{gauss}}(k) = K_{r_{gauss}}(k) - ik<r> = i<r>k-\\frac{<\\Delta r^2>k^2}{2} - ik<r> = -\\frac{<\\Delta r^2>k^2}{2}$$\n",
        "\n",
        "We cannot recover the explicit DKL, so we write:    \n",
        "\n",
        "$$ik(<r>^b - <r>^a) \\le D_{KL}(b\\|a) + K_{r_{gauss}}(k)$$\n",
        "$$ik(<r>^b - <r>^a) \\le D_{KL}(b\\|a) -\\frac{<\\Delta r^2>k^2}{2}$$\n",
        "$$<r>^b - <r>^a \\le \\frac{D_{KL}(b\\|a)}{ik} - \\frac{<\\Delta r^2>k^2}{2ik}$$\n",
        "\n",
        "Having this, we could even obtain the tightest bound by minimising RHS wrt k. I don't know if it's required by the task, but here it goes, I'm curious.\n",
        "\n",
        "$$k_{min} = \\arg \\min_k (\\frac{D_{KL}(b\\|a)}{ik} - \\frac{<\\Delta r^2>k}{2i}) $$\n",
        "\n",
        "The derivative wrt k is $$\\frac{i<\\Delta r^2>}{2}+\\frac{iD_{KL}(b\\|a)}{k^2} = \\frac{i<\\Delta r^2>k^2 + 2iD_{KL}(b\\|a)}{2k^2}$$\n",
        "\n",
        "And the second derivative is $$\\frac{-2iD_{KL}(b\\|a)}{k^3}$$\n",
        "\n",
        "\n",
        "Considering this, the minimum is never attained in the general case, as both squares and KDL are nonnegative, and we consider real $k$. \n",
        "\n",
        "\n",
        "Having obtained permission to use LT instead of FT, we write:    \n",
        "\n",
        "$$K_{r_{gauss}}(k) = \\log\\int_R dre^{kr}P(r) = \\log\\int_R dre^{kr}\\frac{1}{\\sqrt{2\\pi<\\Delta r^2>}}exp(-\\frac{(r-<r>)^2}{2<\\Delta r^2>}) =  \\log\\frac{1}{\\sqrt{2\\pi<\\Delta r^2>}}\\int_R dre^{kr-\\frac{(r-<r>)^2}{2<\\Delta r^2>}}$$\n",
        "$$G_{r_{gauss}}(k) = e^{<r>k+\\frac{<\\Delta r^2>k^2}{2}}$$\n",
        "\n",
        "Therefore, $$K_{r_{gauss}}(k) = \\log G_{r_{gauss}}(k) = <r>k+\\frac{<\\Delta r^2>k^2}{2}$$.\n",
        "$$K_{g_{gauss}}(k) = K_{r_{gauss}}(k) - k<r> = <r>k+\\frac{<\\Delta r^2>k^2}{2} - k<r> = \\frac{<\\Delta r^2>k^2}{2}$$\n",
        "$$<r>^b - <r>^a \\le \\frac{D_{KL}(b\\|a)}{k} + \\frac{<\\Delta r^2>k^2}{2k}$$\n",
        "The derivative wrt k is $$\\frac{<\\Delta r^2>}{2}-\\frac{D_{KL}(b\\|a)}{k^2} = \\frac{<\\Delta r^2>k^2 - 2D_{KL}(b\\|a)}{2k^2}$$\n",
        "\n",
        "And the second derivative is $$\\frac{2D_{KL}(b\\|a)}{k^3}$$\n",
        "\n",
        "Let us assume the second derivative positive. Then $k_{min} = \\pm \\sqrt \\frac{2D_{KL}(b\\|a)}{<\\Delta r^2>}$. \n",
        "\n",
        "And (taking absolutes to simplify) $$|<r>^b - <r>^a| \\le \\frac{D_{KL}(b\\|a)}{|k_{min}|} + \\frac{<\\Delta r^2>k_{min}^2}{2|k_{min}|} = \\frac{D_{KL}(b\\|a)}{|k_{min}|} + \\frac{<\\Delta r^2>(\\frac{2D_{KL}(b\\|a)}{<\\Delta r^2>})}{2|k_{min}|} = \\frac{4D_{KL}(b\\|a)}{2\\sqrt \\frac{2D_{KL}(b\\|a)}{<\\Delta r^2>}} = \\sqrt {2D_{KL}(b\\|a)<\\Delta r^2>}$$\n",
        "\n",
        "Let us now assume the second derivative zero. It necessarily implies $D_{KL}(b\\|a) = 0$, which in turn implies $|<r>^b - <r>^a| = 0$. It is trivial to see that the tightest bound we have obtained holds then as well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTmNuoBm-qsB"
      },
      "source": [
        "# 2. Mutual information\n",
        "\n",
        "Consider random variables x and y, distributed according to a joint probability p(x,y). The mutual information between the two variables is defined by \n",
        "\n",
        "$$I(x,y) = \\sum_{x,y}p(x,y)\\ln{\\frac{p(x,y)}{p_x(x)p_y(y)}}$$\n",
        "\n",
        "Where $p_x(x)$ and $p_y(y)$ are marginal PMFs.\n",
        "\n",
        "1. Relate $I(x,y)$ to the entropies $H(x,y)$,$H(x)$, and $H(y)$ obtained from the corresponding probabilities.\n",
        "2. Calculate the mutual information for the joint Gaussian form\n",
        "\n",
        "$$P(x,y) \\propto exp(-\\frac{ax^2}{2}-\\frac{by^2}{2}-cxy)$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm6zT30gAmac"
      },
      "source": [
        "## Solution\n",
        "\n",
        "1. We know entropy to be equal to \n",
        "$$H(x) = -\\sum_x p(x)\\log p(x)$$\n",
        "$$H(y) = -\\sum_y p(y)\\log p(y)$$\n",
        "$$H(x,y) = -\\sum_{x,y} p(x,y)\\log p(x,y)$$\n",
        "\n",
        "We can turn the quotient under the log into a difference of logs:     \n",
        "$$I(x,y) = \\sum_{x,y}p(x,y)\\log{\\frac{p(x,y)}{p_x(x)p_y(y)}} = \\sum_{x,y}p(x,y)(\\log{p(x,y) - \\log{p_x(x)p_y(y)}}) = \\sum_{x,y}p(x,y)\\log{{p(x,y)}-\\sum_{x,y}p(x,y)\\log{p_x(x)} - \\sum_{x,y}p(x,y)\\log{p_y(y)}}$$\n",
        "\n",
        "Since probabilities are necessrily normalised, we plainly see:      \n",
        "$$\\sum_{x,y}p(x,y)\\log{p_y(y)} = \\sum_{y}p(y)\\log{p_y(y)}$$\n",
        "$$I(x,y) = (-H(x,y))-(-H(y))-(-H(x)) = H(x) + H(y) - H(x,y)$$\n",
        "\n",
        "2. \n",
        "\n",
        "Firstly, we normalise:     \n",
        "$$C\\int_R \\int_R exp(-\\frac{ax^2}{2}-\\frac{by^2}{2}-cxy)dxdy = \\int_R exp(-\\frac{by^2}{2})dy\\int_R exp(-\\frac{ax^2}{2}-cxy)dx = C\\frac{2\\pi}{\\sqrt{ab-c^2}} = 1$$\n",
        "\n",
        "\n",
        "Thus:    \n",
        "\n",
        "$$P(x,y) = \\frac{\\sqrt{ab-c^2}}{2\\pi}exp(-\\frac{ax^2}{2}-\\frac{by^2}{2}-cxy)$$\n",
        "With\n",
        "$$P_x(x) = \\frac{\\sqrt{ab-c^2}}{2\\pi}\\int_R exp(-\\frac{ax^2}{2}-\\frac{by^2}{2}-cxy)dy = \\frac{\\sqrt{ab-c^2}}{2\\pi}exp(-\\frac{a x^2}{2})\\int_R exp(-\\frac{by^2}{2}-cxy)dy = \\frac{\\sqrt{ab-c^2}}{\\sqrt{2\\pi b}}exp(\\frac{(c^2-ab)x^2}{2b})$$\n",
        "And $$P_y(y) = \\frac{\\sqrt{ab-c^2}}{\\sqrt{2\\pi a}}exp(\\frac{(c^2-ab)y^2}{2a})$$\n",
        "\n",
        "Whence\n",
        "\n",
        "$$I(x,y) = \\frac{\\sqrt{ab-c^2}}{2\\pi}\\int_R \\int_R p(x,y)\\log{\\frac{p(x,y)}{p_x(x)p_y(y)}}dxdy = \\int_R \\int_R \\frac{\\sqrt{ab-c^2}}{2\\pi}exp(-\\frac{ax^2}{2}-\\frac{by^2}{2}-cxy)\\log{\\frac{\\sqrt{ab-c^2}}{2\\pi}\\frac{2\\pi \\sqrt{ab}exp(-\\frac{ax^2}{2}-\\frac{by^2}{2}-cxy)}{(ab-c^2)exp(\\frac{(c^2-ab)y^2}{2a}+\\frac{(c^2-ab)x^2}{2b})}}dxdy =  \\int_R \\dfrac{\\sqrt{2}\\sqrt{{\\pi}}a\\sqrt{b}\\ln\\left(\\frac{\\sqrt{ab-c^2}}{2{\\pi}}\\right)\\mathrm{e}^{-\\frac{\\left(c^4-2abc^2+a^2b^2\\right)x^2}{2bc^2+2ab^2}}}{\\sqrt{ab-c^2}\\sqrt{c^2+ab}}dx = \\dfrac{2{\\pi}a\\sqrt{b}\\ln\\left(\\frac{\\sqrt{ab-c^2}}{2{\\pi}}\\right)}{\\sqrt{ab-c^2}\\left(c^2-ab\\right)\\sqrt{c^2+ab}\\sqrt{\\frac{1}{b\\left(c^2+ab\\right)}}}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvGdVWagfLQ4"
      },
      "source": [
        "# 3. Hardy-Weinberg Law\n",
        "\n",
        "Consider an experiment of mating rabbits.  We watch the evolution of a particular gene that appears in two types, G or g.  A rabbit has a pair of genes, either GG (dominant), Gg (hybrid — the order is irrelevant, so gG is the same as Gg) or gg (recessive).  In mating two rabbits, the offspring inherits a  gene  from  each  of  its  parents  with  equal  probability.   Thus,  if  we  mate  a  dominant  (GG)  with  a hybrid (Gg), the offspring is dominant with probability 1/2 or hybrid with probability 1/2.  Start with a rabbit of given character (GG, Gg, or gg) and mate it with a hybrid.  The offspring produced is again mated with a hybrid,  and the process is repeated through a number of generations,  always mating with a hybrid.  Note:  The first experiment of such kind was conducted in 1858 by Gregor Mendel.  He started to breed garden peas in his monastery garden and analysed the offspring of these matings.\n",
        "\n",
        "1. Write  down  the  transition  matrix  P  of  the  Markov  chain  thus  defined. Is  the  Markov chain irreducible and aperiodic?\n",
        "2. Assume  that  we  start  with  a  hybrid  rabbit.   Let μ_n  be  the  probability  distribution  of  the character of the rabbit of the n-th generation.  In other words,μ_n(GG),μ_n(Gg),μ_n(gg) are the probabilities that the n-th generation rabbit is GG, Gg, or gg, respectively.  Compute μ1,μ2,μ3.  Is there some kind of law?\n",
        "3. Calculate P^n for general n.  What can you say about μ_n for general n?\n",
        "4. Calculate the stationary distribution of the Markov chain.  What can you say about the detailed balance?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgGJ24h3fzZR"
      },
      "source": [
        "## Solution\n",
        "\n",
        "1. \\\\\n",
        "\n",
        " - We observe that the transition probability from GG to Gg upon mating with a hybrid is 0.5.\n",
        " - We also observe that the transition probability from gg to Gg upon mating with a hybrid is 0.5.\n",
        " - Finally, we observe that GG and gg do not neighbor each other, and  he transition probability from Gg to either of pure genomes upon mating with a hybrid is 0.25. \n",
        "\n",
        " This lets us construct the transition matrix with the columns corresponding to states GG, gG, gg:     \n",
        " $$\\begin{bmatrix}\n",
        "0.5 & 0.25 & 0\\\\\n",
        "0.5 & 0.5 & 0.5\\\\   \n",
        "0 & 0.25 & 0.5\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "This matrix lacks isolated states and is thus irreducible, as evident from the transition matrix (though, if such is demanded, I may additionally construct a much more easy-to-analyse incidence matrix). \n",
        "\n",
        "Since advancement in time may map a state onto itself for each state, this matrix is aperiodic as all states are manifestly aperiodic. \n",
        "\n",
        "2. \\\\\n",
        "\n",
        "Let us cruelly&callously turn the initial rabbit into a vector:    \n",
        "$$\\mu_0 =  \\begin{bmatrix}\n",
        "0 \\\\\n",
        "1 \\\\   \n",
        "0 \n",
        "\\end{bmatrix}$$\n",
        "\n",
        "By inspection of the matrix we may expect that the rabbit will gravitate towards at least heavily hybrid-dominated state. Let us check:    \n",
        "$$\\mu_1 = P\\mu_0 = \\begin{bmatrix}\n",
        "0.5 & 0.25 & 0\\\\\n",
        "0.5 & 0.5 & 0.5\\\\   \n",
        "0 & 0.25 & 0.5\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "0 \\\\\n",
        "1 \\\\   \n",
        "0 \n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "0.25 \\\\\n",
        "0.5 \\\\   \n",
        "0.25 \n",
        "\\end{bmatrix}$$\n",
        "$$\\mu_2 = P\\mu_1 = \\begin{bmatrix}\n",
        "0.5 & 0.25 & 0\\\\\n",
        "0.5 & 0.5 & 0.5\\\\   \n",
        "0 & 0.25 & 0.5\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "0.25 \\\\\n",
        "0.5 \\\\   \n",
        "0.25 \n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "0.25 \\\\\n",
        "0.5 \\\\   \n",
        "0.25 \n",
        "\\end{bmatrix}$$\n",
        "$$\\mu_3 = P\\mu_2 = \\begin{bmatrix}\n",
        "0.5 & 0.25 & 0\\\\\n",
        "0.5 & 0.5 & 0.5\\\\   \n",
        "0 & 0.25 & 0.5\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "0.25 \\\\\n",
        "0.5 \\\\   \n",
        "0.25 \n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "0.25 \\\\\n",
        "0.5 \\\\   \n",
        "0.25 \n",
        "\\end{bmatrix}$$\n",
        "\n",
        "By now it is quite evident that the rabbit will converge to $\\begin{bmatrix}\n",
        "0.25 \\\\\n",
        "0.5 \\\\   \n",
        "0.25 \n",
        "\\end{bmatrix}$ extremely rapidly and stay there. \n",
        "\n",
        "3. \\\\\n",
        "\n",
        "Firstly, we prove that $\\mu_n = \\begin{bmatrix}\n",
        "0.25 \\\\\n",
        "0.5 \\\\   \n",
        "0.25 \n",
        "\\end{bmatrix} \\forall\\hspace{1mm}n\\ne0$. We do so by the method of induction. \\\\Firstly, we have shown that the base holds in the previous subtask. Let us suppose $\\mu_n = \\begin{bmatrix}\n",
        "0.25 \\\\\n",
        "0.5 \\\\   \n",
        "0.25 \n",
        "\\end{bmatrix}$. Then by  definition of a Markov chain $$\\mu_{n+1} = P\\mu_n = \\begin{bmatrix}\n",
        "0.5 & 0.25 & 0\\\\\n",
        "0.5 & 0.5 & 0.5\\\\   \n",
        "0 & 0.25 & 0.5\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "0.25 \\\\\n",
        "0.5 \\\\   \n",
        "0.25 \n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "0.25 \\\\\n",
        "0.5 \\\\   \n",
        "0.25 \n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$P^1 = \\begin{bmatrix}\n",
        "0.5 & 0.25 & 0\\\\\n",
        "0.5 & 0.5 & 0.5\\\\   \n",
        "0 & 0.25 & 0.5\n",
        "\\end{bmatrix}$$\n",
        "$$P^2 = \\begin{bmatrix}\n",
        "\\frac{3}{8} & 0.25 & \\frac{1}{8}\\\\\n",
        "0.5 & 0.5 & 0.5\\\\   \n",
        "\\frac{1}{8} & 0.25 & \\frac{3}{8}\n",
        "\\end{bmatrix}$$\n",
        "$$P^3 = \\begin{bmatrix}\n",
        "\\frac{5}{16} & 0.25 & \\frac{3}{16}\\\\\n",
        "0.5 & 0.5 & 0.5\\\\   \n",
        "\\frac{3}{16} & 0.25 & \\frac{5}{16}\n",
        "\\end{bmatrix}$$\n",
        "$$P^4 = \\begin{bmatrix}\n",
        "\\frac{9}{32} & 0.25 & \\frac{7}{32}\\\\\n",
        "0.5 & 0.5 & 0.5\\\\   \n",
        "\\frac{7}{32} & 0.25 & \\frac{9}{32}\n",
        "\\end{bmatrix}$$\n",
        "By inspection we can recover:    \n",
        "$$P^n = \\begin{bmatrix}\n",
        "\\frac{2^{n-1}+1}{2^{n+1}} & 0.25 & \\frac{2^{n-1}-1}{2^{n+1}}\\\\\n",
        "0.5 & 0.5 & 0.5\\\\   \n",
        "\\frac{2^{n-1}-1}{2^{n+1}} & 0.25 & \\frac{2^{n-1}+1}{2^{n+1}}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "\n",
        "4. \\\\   \n",
        "We have by Divine Providence already identified such a distribution: it is $$\\begin{bmatrix}\n",
        "0.25 \\\\\n",
        "0.5 \\\\   \n",
        "0.25 \n",
        "\\end{bmatrix}$$, which was proved in previous subtask. \n",
        "\n",
        "Detailed balance condition: $$p_{ji}\\pi_i = p_{ij}\\pi_j$$\n",
        "In other terms, we may construct the ergodicity matrix and demand its symmetricity: $$Q_{ij} = p_{ij}\\pi_j \\rightarrow Q = P\\odot\\Pi; \\Pi = \\begin{bmatrix}\n",
        "\\pi^T \\\\\n",
        "\\pi^T \\\\   \n",
        "\\pi^T \n",
        "\\end{bmatrix}$$\n",
        "$$Q = \\begin{bmatrix}\n",
        "\\frac{1}{8} & \\frac{1}{8} & 0\\\\\n",
        "\\frac{1}{8} & \\frac{1}{4} & \\frac{1}{8}\\\\   \n",
        "0 & \\frac{1}{8} & \\frac{1}{8}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "This matrix is manifestly symmetric. Therefore, the chain is reversible, and the detailed balance holds in the steady state, but, quite obviously, not in the arbitrary state. For instance, it obviously fails in the starting state of subtask 2. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFBNqfPiTMKn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Stochastic hw2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('aqua-predict')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "2120b70050a4bc3015589e0298cdda1a4249879b1ba9304472013b0b8792895a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
